package com.awab.ai

import android.content.Context
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.util.Log
import org.tensorflow.lite.Interpreter
import java.io.File
import java.io.FileInputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import kotlin.math.sqrt
import kotlin.math.ln
import kotlin.math.cos
import kotlin.math.sin
import kotlin.math.exp

class SpeechRecognizer(private val context: Context) {

    private var interpreter: Interpreter? = null
    private var audioRecord: AudioRecord? = null
    private var isRecording = false
    
    private val sampleRate = 16000
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    private val bufferSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat)
    
    // Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø­Ø±Ù - Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ Ù„ÙƒÙˆØ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    // [" ", "Ø£", "Ø¨", "Øª", "Ø«", "Ø¬", "Ø­", "Ø®", "Ø¯", "Ø°", "Ø±", "Ø²", "Ø³", "Ø´", "Øµ", "Ø¶", "Ø·", "Ø¸", "Ø¹", "Øº", "Ù", "Ù‚", "Ùƒ", "Ù„", "Ù…", "Ù†", "Ù‡Ù€", "Ùˆ", "ÙŠ", "Ø©", "Ù‰", "Ø¦", "Ø¡", "Ø¤", "Ø¢", "Ù„Ø§"]
    companion object {
        private const val TAG = "SpeechRecognizer"
        
        // Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø«Ø§Ø¨ØªØ©
        private val CHAR_LIST = listOf(
            " ", "Ø£", "Ø¨", "Øª", "Ø«", "Ø¬", "Ø­", "Ø®", "Ø¯", "Ø°", 
            "Ø±", "Ø²", "Ø³", "Ø´", "Øµ", "Ø¶", "Ø·", "Ø¸", "Ø¹", "Øº", 
            "Ù", "Ù‚", "Ùƒ", "Ù„", "Ù…", "Ù†", "Ù‡Ù€", "Ùˆ", "ÙŠ", 
            "Ø©", "Ù‰", "Ø¦", "Ø¡", "Ø¤", "Ø¢", "Ù„Ø§"
        )
    }
    
    interface RecognitionListener {
        fun onTextRecognized(text: String)
        fun onError(error: String)
        fun onRecordingStarted()
        fun onRecordingStopped()
        fun onVolumeChanged(volume: Float)
        fun onModelLoaded(modelName: String)
    }
    
    private var listener: RecognitionListener? = null
    
    fun setListener(listener: RecognitionListener) {
        this.listener = listener
    }
    
    fun isModelLoaded(): Boolean {
        return interpreter != null
    }

    fun loadModelFromFile(filePath: String): Boolean {
        return try {
            val file = File(filePath)
            if (!file.exists()) {
                Log.e(TAG, "âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: $filePath")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                return false
            }
            
            if (!file.name.endsWith(".tflite")) {
                Log.e(TAG, "âŒ ØµÙŠØºØ© Ø®Ø§Ø·Ø¦Ø©: ${file.name}")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ØµÙŠØºØ© .tflite")
                return false
            }
            
            Log.d(TAG, "ğŸ“‚ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„: ${file.name} (${file.length()} bytes)")
            
            val modelBuffer = loadModelBuffer(file)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            
            interpreter = Interpreter(modelBuffer, options)
            
            // Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            val inputDetails = interpreter?.getInputTensor(0)
            val outputDetails = interpreter?.getOutputTensor(0)
            Log.d(TAG, "ğŸ“Š Input shape: ${inputDetails?.shape()?.contentToString()}")
            Log.d(TAG, "ğŸ“Š Output shape: ${outputDetails?.shape()?.contentToString()}")
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${file.name}")
            listener?.onModelLoaded(file.name)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            Log.e(TAG, "Stack trace:", e)
            listener?.onError("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            false
        }
    }
    
    fun loadModelFromAssets(modelFileName: String = "speech_model.tflite"): Boolean {
        return try {
            val modelBuffer = loadModelFromAssetsBuffer(modelFileName)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            interpreter = Interpreter(modelBuffer, options)
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: $modelFileName")
            listener?.onModelLoaded(modelFileName)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: ${e.message}")
            listener?.onError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ assets")
            false
        }
    }

    private fun loadModelBuffer(file: File): MappedByteBuffer {
        val inputStream = FileInputStream(file)
        val fileChannel = inputStream.channel
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size())
    }
    
    private fun loadModelFromAssetsBuffer(modelFileName: String): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelFileName)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    fun startRecording() {
        if (isRecording) {
            Log.w(TAG, "âš ï¸ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
            return
        }
        
        if (interpreter == null) {
            listener?.onError("ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
            return
        }

        try {
            audioRecord = AudioRecord(
                MediaRecorder.AudioSource.MIC,
                sampleRate,
                channelConfig,
                audioFormat,
                bufferSize
            )

            if (audioRecord?.state != AudioRecord.STATE_INITIALIZED) {
                listener?.onError("ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ")
                return
            }

            isRecording = true
            audioRecord?.startRecording()
            listener?.onRecordingStarted()
            
            Log.d(TAG, "ğŸ¤ Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...")

            Thread {
                recordAndRecognize()
            }.start()

        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            isRecording = false
        }
    }

    fun stopRecording() {
        if (!isRecording) {
            return
        }

        isRecording = false
        
        try {
            audioRecord?.stop()
            audioRecord?.release()
            audioRecord = null
            
            listener?.onRecordingStopped()
            Log.d(TAG, "ğŸ›‘ ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
        }
    }

    private fun recordAndRecognize() {
        val audioBuffer = ShortArray(bufferSize)
        val audioData = mutableListOf<Short>()
        
        val recognizedText = StringBuilder()
        var silenceCount = 0
        val silenceThreshold = 0.01f
        
        // Ù…Ø¯Ø© Ø§Ù„Ù†Ø§ÙØ°Ø©: 2-3 Ø«ÙˆØ§Ù†ÙŠ (ÙƒÙ…Ø§ ÙÙŠ ÙƒÙˆØ¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±)
        val windowDuration = 2.5f // Ø«Ø§Ù†ÙŠØ©
        val windowSize = (sampleRate * windowDuration).toInt()
        
        Log.d(TAG, "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - windowSize: $windowSize, sampleRate: $sampleRate")
        
        try {
            while (isRecording) {
                val readSize = audioRecord?.read(audioBuffer, 0, bufferSize) ?: 0
                
                if (readSize > 0) {
                    // Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª
                    val volume = computeVolume(audioBuffer, readSize)
                    listener?.onVolumeChanged(volume)
                    
                    val isSilent = volume < silenceThreshold
                    
                    if (isSilent) {
                        silenceCount++
                        // Ø¥Ø°Ø§ ØµÙ…Øª Ø·ÙˆÙŠÙ„ (Ø£ÙƒØ«Ø± Ù…Ù† Ø«Ø§Ù†ÙŠØ©) ÙˆØ¹Ù†Ø¯Ù†Ø§ ÙƒÙ„Ø§Ù…ØŒ Ø£Ø±Ø³Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                        if (silenceCount > 30 && recognizedText.isNotEmpty()) {
                            val finalText = recognizedText.toString().trim()
                            Log.d(TAG, "ğŸ“ Ù†Øµ Ù†Ù‡Ø§Ø¦ÙŠ: $finalText")
                            listener?.onTextRecognized(finalText)
                            recognizedText.clear()
                            audioData.clear()
                            silenceCount = 0
                        }
                    } else {
                        silenceCount = 0
                    }
                    
                    // Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø®Ø²Ù†
                    for (i in 0 until readSize) {
                        audioData.add(audioBuffer[i])
                    }
                    
                    // Ø¹Ù†Ø¯ Ø§Ù…ØªÙ„Ø§Ø¡ Ø§Ù„Ù†Ø§ÙØ°Ø©ØŒ Ù‚Ù… Ø¨Ø§Ù„ØªØ¹Ø±Ù
                    if (audioData.size >= windowSize) {
                        val windowData = audioData.take(windowSize).toShortArray()
                        
                        val text = recognizeSpeech(windowData)
                        
                        if (text.isNotBlank()) {
                            recognizedText.append(text)
                            Log.d(TAG, "ğŸ”¤ ØªÙ… Ø§Ù„ØªØ¹Ø±Ù: $text")
                            listener?.onTextRecognized(recognizedText.toString())
                        }
                        
                        // Ù…Ø³Ø­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
                        audioData.clear()
                    }
                }
            }
            
            // Ø¥Ø±Ø³Ø§Ù„ Ø£ÙŠ Ù†Øµ Ù…ØªØ¨Ù‚ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ØªÙˆÙ‚Ù
            if (recognizedText.isNotEmpty()) {
                val finalText = recognizedText.toString().trim()
                Log.d(TAG, "ğŸ“ Ù†Øµ Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ØªÙˆÙ‚Ù: $finalText")
                listener?.onTextRecognized(finalText)
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        }
    }

    private fun computeVolume(buffer: ShortArray, size: Int): Float {
        var sum = 0.0
        for (i in 0 until size) {
            sum += (buffer[i] * buffer[i]).toDouble()
        }
        val rms = sqrt(sum / size)
        return (rms / Short.MAX_VALUE).toFloat()
    }

    /**
     * Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… - Ù…ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ù…Ù†Ø·Ù‚ ÙƒÙˆØ¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
     */
    private fun recognizeSpeech(audioData: ShortArray): String {
        try {
            // 1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ù„Ù€ Spectrogram (Ù…Ø«Ù„ preprocess_audio ÙÙŠ Python)
            val features = preprocessAudio(audioData)
            
            // 2. ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            val inputDetails = interpreter?.getInputTensor(0)
            val inputShape = inputDetails?.shape() ?: return ""
            
            // 3. ØªØ­Ø¶ÙŠØ± buffer Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØµØ­ÙŠØ­
            val batchSize = 1
            val timeSteps = features.data.size
            val nFeatures = if (features.data.isNotEmpty()) features.data[0].size else 0
            
            val inputBuffer = ByteBuffer.allocateDirect(batchSize * timeSteps * nFeatures * 4)
            inputBuffer.order(ByteOrder.nativeOrder())
            
            for (t in 0 until timeSteps) {
                for (f in 0 until nFeatures) {
                    inputBuffer.putFloat(features.data[t][f])
                }
            }
            inputBuffer.rewind()
            
            // resize tensor Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø´ÙƒÙ„ Ù…Ø®ØªÙ„Ù
            val requiredShape = intArrayOf(batchSize, timeSteps, nFeatures)
            interpreter?.resizeInput(0, requiredShape)
            interpreter?.allocateTensors()
            
            // 4. ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            val outputDetails = interpreter?.getOutputTensor(0)
            val outputShape = outputDetails?.shape() ?: return ""
            
            // Ø§Ù„Ù…Ø®Ø±Ø¬ Ù‡Ùˆ Ù…ØµÙÙˆÙØ© Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (indices)
            val outputSize = outputShape.fold(1) { acc, dim -> acc * dim }
            val outputArray = IntArray(outputSize)
            
            // 5. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            interpreter?.run(inputBuffer, outputArray)
            
            // 6. ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± - ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù„Ù†Øµ
            val decodedText = decodeIndices(outputArray)
            
            Log.d(TAG, "ğŸ¯ Indices: ${outputArray.take(20).joinToString()}")
            Log.d(TAG, "ğŸ“ Decoded: $decodedText")
            
            return decodedText
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù: ${e.message}", e)
            return ""
        }
    }
    
    /**
     * Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª - Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù€ preprocess_audio ÙÙŠ Python
     */
    private fun preprocessAudio(audioData: ShortArray): Spectrogram {
        // 1. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØª
        val audio = normalizeAudio(audioData)
        
        // 2. STFT
        val nFFT = 384
        val hopLength = 160
        val winLength = 256
        val stft = computeSTFT(audio, nFFT, hopLength, winLength)
        
        // 3. ØªØ­ÙˆÙŠÙ„ Ù„Ù€ dB
        val specDB = amplitudeToDb(stft)
        
        // 4. Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (spec + 80) / 80
        val normalized = Array(specDB.size) { t ->
            FloatArray(specDB[t].size) { f ->
                (specDB[t][f] + 80f) / 80f
            }
        }
        
        return Spectrogram(
            data = normalized,
            shape = intArrayOf(1, normalized.size, normalized[0].size)
        )
    }
    
    private fun normalizeAudio(audioData: ShortArray): FloatArray {
        // ØªØ­ÙˆÙŠÙ„ Ù„Ù€ float ÙˆØªØ·Ø¨ÙŠØ¹
        val floatData = FloatArray(audioData.size) { i ->
            audioData[i].toFloat() / Short.MAX_VALUE
        }
        
        // Normalize: audio / max(abs(audio))
        val maxAbs = floatData.maxOf { kotlin.math.abs(it) }
        return if (maxAbs > 0) {
            FloatArray(floatData.size) { i -> floatData[i] / maxAbs }
        } else {
            floatData
        }
    }
    
    private fun computeSTFT(audio: FloatArray, nFFT: Int, hopLength: Int, winLength: Int): Array<FloatArray> {
        val numFrames = (audio.size - nFFT) / hopLength + 1
        val fftSize = nFFT / 2 + 1
        
        val stft = Array(numFrames) { FloatArray(fftSize) }
        
        // Hann window
        val window = FloatArray(winLength) { i ->
            0.5f * (1f - cos(2f * Math.PI.toFloat() * i / (winLength - 1)))
        }
        
        for (frame in 0 until numFrames) {
            val start = frame * hopLength
            val fftInput = FloatArray(nFFT) { 0f }
            
            // ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø§ÙØ°Ø©
            for (i in 0 until kotlin.math.min(winLength, audio.size - start)) {
                fftInput[i] = audio[start + i] * window[i]
            }
            
            // FFT Ø¨Ø³ÙŠØ· (Magnitude only)
            for (k in 0 until fftSize) {
                var real = 0f
                var imag = 0f
                
                for (n in 0 until nFFT) {
                    val angle = -2f * Math.PI.toFloat() * k * n / nFFT
                    real += fftInput[n] * cos(angle)
                    imag += fftInput[n] * sin(angle)
                }
                
                // Magnitude
                stft[frame][k] = kotlin.math.sqrt(real * real + imag * imag)
            }
        }
        
        return stft
    }
    
    private fun amplitudeToDb(stft: Array<FloatArray>): Array<FloatArray> {
        val refValue = stft.maxOf { frame -> frame.maxOrNull() ?: 0f }
        
        return Array(stft.size) { t ->
            FloatArray(stft[t].size) { f ->
                val magnitude = stft[t][f]
                20f * ln((magnitude + 1e-10f) / (refValue + 1e-10f)) / ln(10f)
            }
        }
    }
    
    /**
     * ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± - Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù…Ù†Ø·Ù‚ run_test ÙÙŠ Python
     * decoded_text = "".join([char_list[idx-1] for idx in indices if 0 < idx <= len(char_list)])
     */
    private fun decodeIndices(indices: IntArray): String {
        val result = StringBuilder()
        
        for (idx in indices) {
            // idx - 1 Ù„Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹ÙŠØ¯ Ø£Ø±Ù‚Ø§Ù… Ù…Ù† 1 Ø¥Ù„Ù‰ 37
            // 0 ÙŠØ¹Ù†ÙŠ blank/padding
            if (idx > 0 && idx <= CHAR_LIST.size) {
                val char = CHAR_LIST[idx - 1]
                result.append(char)
            }
        }
        
        return result.toString().trim()
    }
    
    private fun floatArrayToByteBuffer(data: Array<FloatArray>): ByteBuffer {
        val totalSize = data.sumOf { it.size }
        val buffer = ByteBuffer.allocateDirect(totalSize * 4)
        buffer.order(ByteOrder.nativeOrder())
        
        for (row in data) {
            for (value in row) {
                buffer.putFloat(value)
            }
        }
        
        buffer.rewind()
        return buffer
    }
    
    fun cleanup() {
        stopRecording()
        interpreter?.close()
        interpreter = null
        Log.d(TAG, "ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
    }
    
    // Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…
    fun release() = cleanup()
    
    // Data class for spectrogram
    private data class Spectrogram(
        val data: Array<FloatArray>,
        val shape: IntArray
    )
}
