package com.awab.ai

import android.content.Context
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.util.Log
import org.tensorflow.lite.Interpreter
import java.io.File
import java.io.FileInputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import kotlin.math.sqrt
import kotlin.math.ln
import kotlin.math.cos

class SpeechRecognizer(private val context: Context) {

    private var interpreter: Interpreter? = null
    private var audioRecord: AudioRecord? = null
    private var isRecording = false
    
    private val sampleRate = 16000
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    private val bufferSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat)
    
    interface RecognitionListener {
        fun onTextRecognized(text: String)
        fun onError(error: String)
        fun onRecordingStarted()
        fun onRecordingStopped()
        fun onVolumeChanged(volume: Float)
        fun onModelLoaded(modelName: String)
    }
    
    private var listener: RecognitionListener? = null
    
    fun setListener(listener: RecognitionListener) {
        this.listener = listener
    }
    
    fun isModelLoaded(): Boolean {
        return interpreter != null
    }

    fun loadModelFromFile(filePath: String): Boolean {
        return try {
            val file = File(filePath)
            if (!file.exists()) {
                Log.e(TAG, "âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: $filePath")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                return false
            }
            
            if (!file.name.endsWith(".tflite")) {
                Log.e(TAG, "âŒ ØµÙŠØºØ© Ø®Ø§Ø·Ø¦Ø©: ${file.name}")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ØµÙŠØºØ© .tflite")
                return false
            }
            
            Log.d(TAG, "ğŸ“‚ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„: ${file.name} (${file.length()} bytes)")
            
            val modelBuffer = loadModelBuffer(file)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            
            interpreter = Interpreter(modelBuffer, options)
            
            val inputDetails = interpreter?.getInputTensor(0)
            val outputDetails = interpreter?.getOutputTensor(0)
            
            Log.d(TAG, "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
            Log.d(TAG, "â•‘ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬                       â•‘")
            Log.d(TAG, "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
            Log.d(TAG, "â•‘ ğŸ“¥ Input: ${inputDetails?.shape()?.contentToString()}")
            Log.d(TAG, "â•‘    Type: ${inputDetails?.dataType()}")
            Log.d(TAG, "â•‘ ğŸ“¤ Output: ${outputDetails?.shape()?.contentToString()}")
            Log.d(TAG, "â•‘    Type: ${outputDetails?.dataType()}")
            Log.d(TAG, "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${file.name}")
            listener?.onModelLoaded(file.name)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}", e)
            listener?.onError("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            false
        }
    }
    
    fun loadModelFromAssets(modelFileName: String = "speech_model.tflite"): Boolean {
        return try {
            val modelBuffer = loadModelFromAssetsBuffer(modelFileName)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            interpreter = Interpreter(modelBuffer, options)
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: $modelFileName")
            listener?.onModelLoaded(modelFileName)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: ${e.message}")
            listener?.onError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ assets")
            false
        }
    }

    private fun loadModelBuffer(file: File): MappedByteBuffer {
        val inputStream = FileInputStream(file)
        val fileChannel = inputStream.channel
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size())
    }
    
    private fun loadModelFromAssetsBuffer(modelFileName: String): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelFileName)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    fun startRecording() {
        if (isRecording) {
            Log.w(TAG, "âš ï¸ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
            return
        }
        
        if (interpreter == null) {
            listener?.onError("ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
            return
        }

        try {
            audioRecord = AudioRecord(
                MediaRecorder.AudioSource.MIC,
                sampleRate,
                channelConfig,
                audioFormat,
                bufferSize
            )

            if (audioRecord?.state != AudioRecord.STATE_INITIALIZED) {
                listener?.onError("ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ")
                return
            }

            isRecording = true
            audioRecord?.startRecording()
            listener?.onRecordingStarted()
            
            Log.d(TAG, "ğŸ¤ Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...")

            Thread {
                recordAndRecognize()
            }.start()

        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            isRecording = false
        }
    }

    fun stopRecording() {
        if (!isRecording) {
            return
        }

        isRecording = false
        
        try {
            audioRecord?.stop()
            audioRecord?.release()
            audioRecord = null
            
            listener?.onRecordingStopped()
            Log.d(TAG, "ğŸ›‘ ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
        }
    }

    private fun recordAndRecognize() {
        val audioBuffer = ShortArray(bufferSize)
        val audioData = mutableListOf<Short>()
        
        val recognizedText = StringBuilder()
        var silenceCount = 0
        val silenceThreshold = 0.01f
        
        val windowDuration = 0.5f
        val windowSize = (sampleRate * windowDuration).toInt()
        val overlapRatio = 0.5f
        val hopSize = (windowSize * (1 - overlapRatio)).toInt()
        
        Log.d(TAG, "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - windowSize: $windowSize, hopSize: $hopSize")
        
        try {
            while (isRecording) {
                val readSize = audioRecord?.read(audioBuffer, 0, bufferSize) ?: 0
                
                if (readSize > 0) {
                    val volume = computeVolume(audioBuffer, readSize)
                    listener?.onVolumeChanged(volume)
                    
                    val isSilent = volume < silenceThreshold
                    
                    if (isSilent) {
                        silenceCount++
                        if (silenceCount > 10 && recognizedText.isNotEmpty()) {
                            if (recognizedText.last() != ' ') {
                                recognizedText.append(" ")
                                listener?.onTextRecognized(recognizedText.toString())
                            }
                        }
                    } else {
                        silenceCount = 0
                    }
                    
                    for (i in 0 until readSize) {
                        audioData.add(audioBuffer[i])
                    }
                    
                    while (audioData.size >= windowSize) {
                        val windowData = audioData.take(windowSize).toShortArray()
                        
                        if (!isSilent) {
                            val text = recognizeSpeech(windowData)
                            
                            if (text.isNotBlank()) {
                                recognizedText.append(text)
                                listener?.onTextRecognized(recognizedText.toString())
                            }
                        }
                        
                        val toRemove = kotlin.math.min(hopSize, audioData.size)
                        repeat(toRemove) { audioData.removeAt(0) }
                    }
                }
            }
            
            if (audioData.size >= windowSize / 2 && recognizedText.isNotEmpty()) {
                val remainingData = audioData.toShortArray()
                val text = recognizeSpeech(remainingData)
                if (text.isNotBlank()) {
                    recognizedText.append(text)
                }
            }
            
            if (recognizedText.isNotEmpty()) {
                val finalText = recognizedText.toString().trim()
                Log.d(TAG, "ğŸ“ Ù†Øµ Ù†Ù‡Ø§Ø¦ÙŠ: $finalText")
                listener?.onTextRecognized(finalText)
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        }
    }

    private fun computeVolume(buffer: ShortArray, size: Int): Float {
        var sum = 0.0
        for (i in 0 until size) {
            sum += (buffer[i] * buffer[i]).toDouble()
        }
        val rms = sqrt(sum / size)
        return (rms / Short.MAX_VALUE).toFloat()
    }

    // ========== Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„ØªØ¹Ø±Ù - Ù…Ø¨Ø³Ø·Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© ==========
    
    /**
     * Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…
     * Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨ÙƒÙ„ Ø´ÙŠØ¡ - Ù†Ø­Ù† ÙÙ‚Ø· Ù†ØªØ±Ø¬Ù… Ø§Ù„Ù€ indices
     */
    private fun recognizeSpeech(audioData: ShortArray): String {
        try {
            // 1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ù„Ù€ Spectrogram [1, time, 257]
            val spectrogram = audioToSpectrogram(audioData)
            
            // 2. ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            interpreter?.resizeInput(0, spectrogram.shape)
            interpreter?.allocateTensors()
            
            val inputBuffer = createInputBuffer(spectrogram)
            
            // 3. ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            val outputDetails = interpreter?.getOutputTensor(0)
            val tensorSize = outputDetails?.numBytes() ?: 0
            
            if (tensorSize == 0) {
                Log.e(TAG, "âŒ Ø­Ø¬Ù… Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª = 0")
                return ""
            }
            
            val outputBuffer = ByteBuffer.allocateDirect(tensorSize)
            outputBuffer.order(ByteOrder.nativeOrder())
            
            // 4. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            interpreter?.run(inputBuffer, outputBuffer)
            
            // 5. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            outputBuffer.rewind()
            
            val dataType = outputDetails?.dataType()
            Log.d(TAG, "ğŸ“¤ Output type: $dataType, size: $tensorSize bytes")
            
            val text = when (dataType) {
                org.tensorflow.lite.DataType.INT32 -> {
                    val numElements = tensorSize / 4
                    val indices = IntArray(numElements) {
                        outputBuffer.int
                    }
                    Log.d(TAG, "ğŸ“Š Indices (${indices.size}): ${indices.joinToString()}")
                    decodeIndices(indices)
                }
                org.tensorflow.lite.DataType.INT64 -> {
                    val numElements = tensorSize / 8
                    val indices = IntArray(numElements) {
                        outputBuffer.long.toInt()
                    }
                    Log.d(TAG, "ğŸ“Š Indices (${indices.size}): ${indices.joinToString()}")
                    decodeIndices(indices)
                }
                else -> {
                    Log.e(TAG, "âŒ Ù†ÙˆØ¹ Ù…Ø®Ø±Ø¬Ø§Øª ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: $dataType")
                    ""
                }
            }
            
            if (text.isNotBlank()) {
                Log.d(TAG, "âœ… Ø§Ù„Ù†Øµ: '$text'")
            }
            
            return text
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù: ${e.message}", e)
            return ""
        }
    }
    
    /**
     * ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Spectrogram
     * Input: ShortArray (raw audio)
     * Output: [1, time, 257] spectrogram
     */
    private fun audioToSpectrogram(audioData: ShortArray): Spectrogram {
        // 1. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØª
        val audio = normalizeAudio(audioData)
        
        // 2. STFT Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        val nFFT = 512
        val hopLength = 128
        val winLength = 400
        val stft = computeSTFT(audio, nFFT, hopLength, winLength)
        
        // 3. ØªØ­ÙˆÙŠÙ„ Ù„Ù€ dB
        val specDB = amplitudeToDb(stft)
        
        // 4. Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (spec + 80) / 80
        val normalized = Array(specDB.size) { t ->
            FloatArray(specDB[t].size) { f ->
                (specDB[t][f] + 80f) / 80f
            }
        }
        
        return Spectrogram(
            data = normalized,
            shape = intArrayOf(1, normalized.size, normalized[0].size)
        )
    }
    
    private fun normalizeAudio(audioData: ShortArray): FloatArray {
        val floatData = FloatArray(audioData.size) { i ->
            audioData[i].toFloat() / Short.MAX_VALUE
        }
        
        val maxAbs = floatData.maxOf { kotlin.math.abs(it) }
        return if (maxAbs > 0f) {
            FloatArray(floatData.size) { i -> floatData[i] / maxAbs }
        } else {
            floatData
        }
    }
    
    private fun computeSTFT(audio: FloatArray, nFFT: Int, hopLength: Int, winLength: Int): Array<FloatArray> {
        val numFrames = (audio.size - nFFT) / hopLength + 1
        val fftSize = nFFT / 2 + 1
        val stft = Array(numFrames) { FloatArray(fftSize) }
        
        val window = FloatArray(winLength) { i ->
            0.5f * (1f - cos(2f * Math.PI.toFloat() * i / (winLength - 1)))
        }
        
        for (frame in 0 until numFrames) {
            val start = frame * hopLength
            val fftInput = FloatArray(nFFT) { 0f }
            
            for (i in 0 until kotlin.math.min(winLength, audio.size - start)) {
                fftInput[i] = audio[start + i] * window[i]
            }
            
            for (k in 0 until fftSize) {
                var real = 0f
                var imag = 0f
                
                for (n in 0 until nFFT) {
                    val angle = -2f * Math.PI.toFloat() * k * n / nFFT
                    real += fftInput[n] * cos(angle)
                    imag += fftInput[n] * kotlin.math.sin(angle)
                }
                
                stft[frame][k] = kotlin.math.sqrt(real * real + imag * imag)
            }
        }
        
        return stft
    }
    
    private fun amplitudeToDb(stft: Array<FloatArray>): Array<FloatArray> {
        val refValue = stft.maxOf { frame -> frame.maxOrNull() ?: 0f }
        
        return Array(stft.size) { t ->
            FloatArray(stft[t].size) { f ->
                val magnitude = stft[t][f]
                20f * ln((magnitude + 1e-10f) / (refValue + 1e-10f)) / ln(10f)
            }
        }
    }
    
    /**
     * ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ù€ Indices - Ù…Ø¨Ø§Ø´Ø± ÙˆØ¨Ø³ÙŠØ·
     * Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ø®Ø±Ø¬ indices Ø¬Ø§Ù‡Ø²Ø©ØŒ Ù†Ø­Ù† ÙÙ‚Ø· Ù†ØªØ±Ø¬Ù…Ù‡Ø§
     */
    private fun decodeIndices(indices: IntArray): String {
        val vocabulary = loadVocabulary()
        val result = StringBuilder()
        
        for (idx in indices) {
            // Ø§Ù„Ù€ indices ØªØ¨Ø¯Ø£ Ù…Ù† 0
            // 0 = Ù…Ø³Ø§ÙØ© (Ø£Ùˆ Ø£ÙˆÙ„ Ø­Ø±Ù ÙÙŠ vocabulary)
            // 1-31 = Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø­Ø±Ù
            
            if (idx >= 0 && idx < vocabulary.size) {
                val char = vocabulary[idx]
                result.append(char)
                Log.d(TAG, "  idx=$idx â†’ char='$char'")
            } else {
                Log.w(TAG, "  idx=$idx â†’ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ (vocabulary size=${vocabulary.size})")
            }
        }
        
        return result.toString()
    }
    
    private fun createInputBuffer(spectrogram: Spectrogram): ByteBuffer {
        val totalSize = spectrogram.data.sumOf { it.size }
        val buffer = ByteBuffer.allocateDirect(totalSize * 4)
        buffer.order(ByteOrder.nativeOrder())
        
        for (row in spectrogram.data) {
            for (value in row) {
                buffer.putFloat(value)
            }
        }
        
        buffer.rewind()
        return buffer
    }

    /**
     * ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
     * Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ = Ù…Ø³Ø§ÙØ© (index 0)
     * Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø³Ø·Ø± = Ø§Ù„Ø£Ø­Ø±Ù (indices 1-31)
     */
    private fun loadVocabulary(): List<String> {
        return try {
            val lines = context.assets.open("vocabulary.txt").bufferedReader().readLines()
            Log.d(TAG, "ğŸ“– ØªÙ… ØªØ­Ù…ÙŠÙ„ ${lines.size} Ø­Ø±Ù Ù…Ù† vocabulary.txt")
            lines
        } catch (e: Exception) {
            Log.w(TAG, "âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ vocabulary.txtØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
            // Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© - ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ø³Ø§ÙØ© ÙÙŠ Ø§Ù„Ø£ÙˆÙ„
            listOf(
                " ", "Ø§", "Ø¨", "Øª", "Ø«", "Ø¬", "Ø­", "Ø®", "Ø¯", "Ø°", 
                "Ø±", "Ø²", "Ø³", "Ø´", "Øµ", "Ø¶", "Ø·", "Ø¸", "Ø¹", "Øº", 
                "Ù", "Ù‚", "Ùƒ", "Ù„", "Ù…", "Ù†", "Ù‡Ù€", "Ùˆ", "ÙŠ", 
                "Ù‰", "Ø¦", "Ø¤"
            )
        }
    }

    fun cleanup() {
        stopRecording()
        interpreter?.close()
        interpreter = null
        Log.d(TAG, "ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
    }

    companion object {
        private const val TAG = "SpeechRecognizer"
    }
    
    private data class Spectrogram(
        val data: Array<FloatArray>,
        val shape: IntArray
    )
}
