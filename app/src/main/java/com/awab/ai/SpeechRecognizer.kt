package com.awab.ai

import android.content.Context
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.util.Log
import org.tensorflow.lite.Interpreter
import java.io.File
import java.io.FileInputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import kotlin.math.sqrt
import kotlin.math.ln
import kotlin.math.cos

class SpeechRecognizer(private val context: Context) {

    private var interpreter: Interpreter? = null
    private var audioRecord: AudioRecord? = null
    private var isRecording = false
    
    private val sampleRate = 16000
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    private val bufferSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat)
    private val inputSize = 16000
    
    interface RecognitionListener {
        fun onTextRecognized(text: String)
        fun onError(error: String)
        fun onRecordingStarted()
        fun onRecordingStopped()
        fun onVolumeChanged(volume: Float)
        fun onModelLoaded(modelName: String)
    }
    
    private var listener: RecognitionListener? = null
    
    fun setListener(listener: RecognitionListener) {
        this.listener = listener
    }
    
    /**
     * Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
     */
    fun isModelLoaded(): Boolean {
        return interpreter != null
    }

    /**
     * ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ (Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‡Ø§ØªÙ)
     */
    fun loadModelFromFile(filePath: String): Boolean {
        return try {
            val file = File(filePath)
            if (!file.exists()) {
                Log.e(TAG, "âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: $filePath")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                return false
            }
            
            if (!file.name.endsWith(".tflite")) {
                Log.e(TAG, "âŒ ØµÙŠØºØ© Ø®Ø§Ø·Ø¦Ø©: ${file.name}")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ØµÙŠØºØ© .tflite")
                return false
            }
            
            Log.d(TAG, "ğŸ“‚ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„: ${file.name} (${file.length()} bytes)")
            
            val modelBuffer = loadModelBuffer(file)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            
            Log.d(TAG, "ğŸ”§ Ø¥Ù†Ø´Ø§Ø¡ Interpreter...")
            interpreter = Interpreter(modelBuffer, options)
            
            // Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            val inputDetails = interpreter?.getInputTensor(0)
            val outputDetails = interpreter?.getOutputTensor(0)
            
            Log.d(TAG, "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
            Log.d(TAG, "â•‘ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬                       â•‘")
            Log.d(TAG, "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
            Log.d(TAG, "â•‘ ğŸ“¥ Input:                              â•‘")
            Log.d(TAG, "â•‘   Shape: ${inputDetails?.shape()?.contentToString()}")
            Log.d(TAG, "â•‘   Type: ${inputDetails?.dataType()}")
            Log.d(TAG, "â•‘                                        â•‘")
            Log.d(TAG, "â•‘ ğŸ“¤ Output:                             â•‘")
            Log.d(TAG, "â•‘   Shape: ${outputDetails?.shape()?.contentToString()}")
            Log.d(TAG, "â•‘   Type: ${outputDetails?.dataType()}")
            Log.d(TAG, "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${file.name}")
            listener?.onModelLoaded(file.name)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            Log.e(TAG, "Stack trace:", e)
            listener?.onError("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            false
        }
    }
    
    /**
     * ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
     */
    fun loadModelFromAssets(modelFileName: String = "speech_model.tflite"): Boolean {
        return try {
            val modelBuffer = loadModelFromAssetsBuffer(modelFileName)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            interpreter = Interpreter(modelBuffer, options)
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: $modelFileName")
            listener?.onModelLoaded(modelFileName)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: ${e.message}")
            listener?.onError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ assets")
            false
        }
    }

    private fun loadModelBuffer(file: File): MappedByteBuffer {
        val inputStream = FileInputStream(file)
        val fileChannel = inputStream.channel
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size())
    }
    
    private fun loadModelFromAssetsBuffer(modelFileName: String): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelFileName)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    fun startRecording() {
        if (isRecording) {
            Log.w(TAG, "âš ï¸ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
            return
        }
        
        if (interpreter == null) {
            listener?.onError("ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
            return
        }

        try {
            audioRecord = AudioRecord(
                MediaRecorder.AudioSource.MIC,
                sampleRate,
                channelConfig,
                audioFormat,
                bufferSize
            )

            if (audioRecord?.state != AudioRecord.STATE_INITIALIZED) {
                listener?.onError("ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ")
                return
            }

            isRecording = true
            audioRecord?.startRecording()
            listener?.onRecordingStarted()
            
            Log.d(TAG, "ğŸ¤ Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...")

            Thread {
                recordAndRecognize()
            }.start()

        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            isRecording = false
        }
    }

    fun stopRecording() {
        if (!isRecording) {
            return
        }

        isRecording = false
        
        try {
            audioRecord?.stop()
            audioRecord?.release()
            audioRecord = null
            
            listener?.onRecordingStopped()
            Log.d(TAG, "ğŸ›‘ ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
        }
    }

    private fun recordAndRecognize() {
        val audioBuffer = ShortArray(bufferSize)
        val audioData = mutableListOf<Short>()
        
        val recognizedText = StringBuilder()
        var silenceCount = 0
        val silenceThreshold = 0.01f
        
        // Ù†Ø§ÙØ°Ø© Ø£ØµØºØ± Ù„Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© - 0.5 Ø«Ø§Ù†ÙŠØ© ÙÙ‚Ø·
        val windowDuration = 0.5f
        val windowSize = (sampleRate * windowDuration).toInt()
        
        // overlap Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„
        val overlapRatio = 0.5f
        val hopSize = (windowSize * (1 - overlapRatio)).toInt()
        
        Log.d(TAG, "ğŸ“Š Ø¨Ø¯Ø¡ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - windowSize: $windowSize, hopSize: $hopSize, sampleRate: $sampleRate")
        
        try {
            while (isRecording) {
                val readSize = audioRecord?.read(audioBuffer, 0, bufferSize) ?: 0
                
                if (readSize > 0) {
                    val volume = computeVolume(audioBuffer, readSize)
                    listener?.onVolumeChanged(volume)
                    
                    val isSilent = volume < silenceThreshold
                    
                    if (isSilent) {
                        silenceCount++
                        // Ø¥Ø°Ø§ ØµÙ…Øª Ø·ÙˆÙŠÙ„ØŒ Ø£Ø¶Ù Ù…Ø³Ø§ÙØ©
                        if (silenceCount > 10 && recognizedText.isNotEmpty()) {
                            if (recognizedText.last() != ' ') {
                                recognizedText.append(" ")
                                listener?.onTextRecognized(recognizedText.toString())
                            }
                        }
                    } else {
                        silenceCount = 0
                    }
                    
                    // Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    for (i in 0 until readSize) {
                        audioData.add(audioBuffer[i])
                    }
                    
                    // Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ø¹Ù†Ø¯ ÙˆØµÙˆÙ„ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„ØµØºÙŠØ±Ø©
                    while (audioData.size >= windowSize) {
                        val windowData = audioData.take(windowSize).toShortArray()
                        
                        // Ø§Ù„ØªØ¹Ø±Ù Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† ØµÙ…Øª
                        if (!isSilent) {
                            val text = recognizeSpeech(windowData)
                            
                            if (text.isNotBlank()) {
                                recognizedText.append(text)
                                Log.d(TAG, "ğŸ”¤ ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ù…Ø¨Ø§Ø´Ø±Ø©: $text")
                                listener?.onTextRecognized(recognizedText.toString())
                            }
                        }
                        
                        // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ overlap
                        val toRemove = kotlin.math.min(hopSize, audioData.size)
                        repeat(toRemove) { audioData.removeAt(0) }
                    }
                }
            }
            
            // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¨Ù‚ÙŠØ©
            if (audioData.size >= windowSize / 2 && recognizedText.isNotEmpty()) {
                val remainingData = audioData.toShortArray()
                val text = recognizeSpeech(remainingData)
                if (text.isNotBlank()) {
                    recognizedText.append(text)
                }
            }
            
            // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if (recognizedText.isNotEmpty()) {
                val finalText = recognizedText.toString().trim()
                Log.d(TAG, "ğŸ“ Ù†Øµ Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ØªÙˆÙ‚Ù: $finalText")
                listener?.onTextRecognized(finalText)
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        }
    }

    private fun computeVolume(buffer: ShortArray, size: Int): Float {
        var sum = 0.0
        for (i in 0 until size) {
            sum += (buffer[i] * buffer[i]).toDouble()
        }
        val rms = sqrt(sum / size)
        return (rms / Short.MAX_VALUE).toFloat()
    }

    // ========== Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªÙŠØ© - Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ ==========
    
    private fun recognizeSpeech(audioData: ShortArray): String {
        try {
            // 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª Ù…Ø«Ù„ prepare_audio ÙÙŠ Python
            val features = prepareAudioLikeLibrosa(audioData)
            
            // 2. Resize tensor
            val inputDetails = interpreter?.getInputTensor(0)
            interpreter?.resizeInput(0, features.shape)
            interpreter?.allocateTensors()
            
            // 3. ØªØ­ÙˆÙŠÙ„ Ù„Ù€ ByteBuffer
            val inputBuffer = createInputBuffer(features)
            
            // 4. ÙØ­Øµ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            val outputDetails = interpreter?.getOutputTensor(0)
            val outputShape = outputDetails?.shape()
            
            if (outputShape == null || outputShape.isEmpty()) {
                Log.e(TAG, "âŒ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª null Ø£Ùˆ ÙØ§Ø±Øº")
                return ""
            }
            
            // Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø´ÙƒÙ„ Ù„Ù„ØªØ´Ø®ÙŠØµ
            Log.d(TAG, "ğŸ“Š Output shape: ${outputShape.contentToString()}")
            
            // 5. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø­Ø³Ø¨ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            val text = when (outputShape.size) {
                1 -> {
                    // Ø´ÙƒÙ„: [total_elements]
                    // Ø§Ø³ØªØ®Ø¯Ø§Ù… ByteBuffer Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† IntArray
                    val tensorSize = outputDetails.numBytes()
                    val outputBuffer = ByteBuffer.allocateDirect(tensorSize)
                    outputBuffer.order(ByteOrder.nativeOrder())
                    
                    interpreter?.run(inputBuffer, outputBuffer)
                    
                    // Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    outputBuffer.rewind()
                    
                    // ÙØ­Øµ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    when (outputDetails.dataType()) {
                        org.tensorflow.lite.DataType.FLOAT32 -> {
                            val numElements = tensorSize / 4 // 4 bytes per float
                            val floatArray = FloatArray(numElements) {
                                outputBuffer.float
                            }
                            Log.d(TAG, "ğŸ“Š Float output: ${floatArray.take(13).joinToString()}")
                            decodeFloatArray(floatArray)
                        }
                        org.tensorflow.lite.DataType.INT32 -> {
                            val numElements = tensorSize / 4 // 4 bytes per int
                            val intArray = IntArray(numElements) {
                                outputBuffer.int
                            }
                            Log.d(TAG, "ğŸ“Š Int output: ${intArray.take(13).joinToString()}")
                            decodeIndicesArray(intArray)
                        }
                        org.tensorflow.lite.DataType.INT64 -> {
                            val numElements = tensorSize / 8 // 8 bytes per long
                            val longArray = LongArray(numElements) {
                                outputBuffer.long
                            }
                            val intArray = longArray.map { it.toInt() }.toIntArray()
                            Log.d(TAG, "ğŸ“Š Long output: ${intArray.take(13).joinToString()}")
                            decodeIndicesArray(intArray)
                        }
                        else -> {
                            Log.e(TAG, "âŒ Ù†ÙˆØ¹ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: ${outputDetails.dataType()}")
                            ""
                        }
                    }
                }
                2 -> {
                    // Ø´ÙƒÙ„: [time, vocab] Ø£Ùˆ [batch*time, vocab]
                    val timeSteps = outputShape[0]
                    val vocabSize = outputShape[1]
                    val outputArray = Array(timeSteps) { FloatArray(vocabSize) }
                    interpreter?.run(inputBuffer, outputArray)
                    ctcDecodeGreedy(outputArray)
                }
                3 -> {
                    // Ø´ÙƒÙ„: [batch, time, vocab]
                    val batchSize = outputShape[0]
                    val timeSteps = outputShape[1]
                    val vocabSize = outputShape[2]
                    val outputArray = Array(batchSize) { Array(timeSteps) { FloatArray(vocabSize) } }
                    interpreter?.run(inputBuffer, outputArray)
                    ctcDecodeGreedy(outputArray[0])
                }
                else -> {
                    Log.e(TAG, "âŒ Ø´ÙƒÙ„ Ù…Ø®Ø±Ø¬Ø§Øª ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: ${outputShape.size} Ø£Ø¨Ø¹Ø§Ø¯")
                    ""
                }
            }
            
            if (text.isNotBlank()) {
                Log.d(TAG, "ğŸ“ Decoded text: $text")
            }
            
            return text
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù: ${e.message}", e)
            e.printStackTrace()
            return ""
        }
    }
    
    /**
     * Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª - Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù€ prepare_audio ÙÙŠ Python:
     * - librosa.load(sr=16000)
     * - librosa.util.normalize(audio)
     * - librosa.stft(n_fft=512, hop_length=128, win_length=400)
     * - librosa.amplitude_to_db()
     * - (spec + 80) / 80
     */
    private fun prepareAudioLikeLibrosa(audioData: ShortArray): ProcessedAudio {
        // 1. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØª (librosa.util.normalize)
        val audio = normalizeAudio(audioData)
        
        // 2. STFT - Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
        val nFFT = 512
        val hopLength = 128
        val winLength = 400
        val stft = computeSTFT(audio, nFFT, hopLength, winLength)
        
        // 3. amplitude_to_db
        val specDB = amplitudeToDb(stft)
        
        // 4. Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (spec + 80) / 80
        val normalizedSpec = Array(specDB.size) { t ->
            FloatArray(specDB[t].size) { f ->
                (specDB[t][f] + 80f) / 80f
            }
        }
        
        // Shape: [1, time, freq]
        return ProcessedAudio(
            data = normalizedSpec,
            shape = intArrayOf(1, normalizedSpec.size, normalizedSpec[0].size)
        )
    }
    
    private fun normalizeAudio(audioData: ShortArray): FloatArray {
        val floatData = FloatArray(audioData.size) { i ->
            audioData[i].toFloat() / Short.MAX_VALUE
        }
        
        // librosa.util.normalize: audio / max(abs(audio))
        val maxAbs = floatData.maxOf { kotlin.math.abs(it) }
        return if (maxAbs > 0f) {
            FloatArray(floatData.size) { i -> floatData[i] / maxAbs }
        } else {
            floatData
        }
    }
    
    private fun computeSTFT(audio: FloatArray, nFFT: Int, hopLength: Int, winLength: Int): Array<FloatArray> {
        val numFrames = (audio.size - nFFT) / hopLength + 1
        val fftSize = nFFT / 2 + 1
        
        val stft = Array(numFrames) { FloatArray(fftSize) }
        
        // Hann window
        val window = FloatArray(winLength) { i ->
            0.5f * (1f - cos(2f * Math.PI.toFloat() * i / (winLength - 1)))
        }
        
        for (frame in 0 until numFrames) {
            val start = frame * hopLength
            val fftInput = FloatArray(nFFT) { 0f }
            
            // ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø§ÙØ°Ø©
            for (i in 0 until kotlin.math.min(winLength, audio.size - start)) {
                fftInput[i] = audio[start + i] * window[i]
            }
            
            // FFT - Magnitude only
            for (k in 0 until fftSize) {
                var real = 0f
                var imag = 0f
                
                for (n in 0 until nFFT) {
                    val angle = -2f * Math.PI.toFloat() * k * n / nFFT
                    real += fftInput[n] * cos(angle)
                    imag += fftInput[n] * kotlin.math.sin(angle)
                }
                
                stft[frame][k] = kotlin.math.sqrt(real * real + imag * imag)
            }
        }
        
        return stft
    }
    
    private fun amplitudeToDb(stft: Array<FloatArray>): Array<FloatArray> {
        val refValue = stft.maxOf { frame -> frame.maxOrNull() ?: 0f }
        
        return Array(stft.size) { t ->
            FloatArray(stft[t].size) { f ->
                val magnitude = stft[t][f]
                20f * ln((magnitude + 1e-10f) / (refValue + 1e-10f)) / ln(10f)
            }
        }
    }
    
    /**
     * ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± CTC - Ù…Ø·Ø§Ø¨Ù‚ Ù„ÙƒÙˆØ¯ Python Ø§Ù„Ø¬Ø¯ÙŠØ¯:
     * predictions = np.argmax(logits, axis=-1)[0]
     * Ø«Ù… Ø­Ø°Ù Ø§Ù„Ù…ÙƒØ±Ø± ÙˆØ§Ù„Ù€ blank
     */
    private fun ctcDecodeGreedy(logits: Array<FloatArray>): String {
        val vocabulary = loadVocabulary()
        val blankIndex = vocabulary.size // Ø§Ù„Ù€ blank ÙŠÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        
        val result = StringBuilder()
        var lastChar = -1
        
        // argmax Ø¹Ù„Ù‰ ÙƒÙ„ timestep
        for (t in logits.indices) {
            val probs = logits[t]
            
            // Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù€ index Ø§Ù„Ø£ÙƒØ¨Ø±
            var maxIdx = 0
            var maxProb = Float.MIN_VALUE
            
            for (i in probs.indices) {
                if (probs[i] > maxProb) {
                    maxProb = probs[i]
                    maxIdx = i
                }
            }
            
            // CTC rules: Ø­Ø°Ù Ø§Ù„Ù…ÙƒØ±Ø± ÙˆØ­Ø°Ù Ø§Ù„Ù€ blank
            if (maxIdx != lastChar && maxIdx != blankIndex) {
                if (maxIdx < vocabulary.size) {
                    result.append(vocabulary[maxIdx])
                }
            }
            
            lastChar = maxIdx
        }
        
        return result.toString().trim()
    }
    
    /**
     * ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ù…Ù† Ù…ØµÙÙˆÙØ© indices Ø¬Ø§Ù‡Ø²Ø© Ù…Ù† CTC decoder
     * (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙØ±Ø¬Ø¹ indices Ù…Ù† 1 Ø¥Ù„Ù‰ 32ØŒ ØªÙ… ÙÙƒ CTC Ø¨Ø§Ù„ÙØ¹Ù„)
     */
    private fun decodeIndicesArray(indices: IntArray): String {
        val vocabulary = loadVocabulary()
        val result = StringBuilder()
        
        for (idx in indices) {
            // Ø§Ù„Ù€ indices Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªØªØ±Ø§ÙˆØ­ Ù…Ù† 1 Ø¥Ù„Ù‰ 32
            // Ù†Ø·Ø±Ø­ 1 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ index ÙÙŠ vocabulary (0-31)
            val vocabIndex = idx - 1
            
            if (vocabIndex >= 0 && vocabIndex < vocabulary.size) {
                result.append(vocabulary[vocabIndex])
            }
        }
        
        return result.toString().trim()
    }
    
    /**
     * ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ù…Ù† Ù…ØµÙÙˆÙØ© floats (logits)
     * Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ØªÙØ±Ø¬Ø¹ probabilities Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† indices
     */
    private fun decodeFloatArray(floats: FloatArray): String {
        val vocabulary = loadVocabulary()
        val result = StringBuilder()
        
        // Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØµÙÙˆÙØ© logitsØŒ Ù†Ø£Ø®Ø° argmax
        // Ø«Ù… Ù†Ø·Ø¨Ù‚ CTC rules
        var lastChar = -1
        val blankIndex = vocabulary.size
        
        val chunkSize = vocabulary.size + 1 // vocab + blank
        val numTimeSteps = floats.size / chunkSize
        
        for (t in 0 until numTimeSteps) {
            val startIdx = t * chunkSize
            val endIdx = startIdx + chunkSize
            
            if (endIdx <= floats.size) {
                // argmax
                var maxIdx = 0
                var maxProb = Float.MIN_VALUE
                
                for (i in 0 until chunkSize) {
                    val prob = floats[startIdx + i]
                    if (prob > maxProb) {
                        maxProb = prob
                        maxIdx = i
                    }
                }
                
                // CTC rules
                if (maxIdx != lastChar && maxIdx != blankIndex && maxIdx < vocabulary.size) {
                    result.append(vocabulary[maxIdx])
                }
                
                lastChar = maxIdx
            }
        }
        
        return result.toString().trim()
    }
    
    private fun createInputBuffer(features: ProcessedAudio): ByteBuffer {
        val totalSize = features.data.sumOf { it.size }
        val buffer = ByteBuffer.allocateDirect(totalSize * 4)
        buffer.order(ByteOrder.nativeOrder())
        
        for (row in features.data) {
            for (value in row) {
                buffer.putFloat(value)
            }
        }
        
        buffer.rewind()
        return buffer
    }

    private fun loadVocabulary(): List<String> {
        return try {
            context.assets.open("vocabulary.txt").bufferedReader().readLines()
        } catch (e: Exception) {
            Log.w(TAG, "âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¦Ù…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
            // Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© - Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù€ char_list ÙÙŠ Python
            listOf(
                " ", "Ø§", "Ø¨", "Øª", "Ø«", "Ø¬", "Ø­", "Ø®", "Ø¯", "Ø°", 
                "Ø±", "Ø²", "Ø³", "Ø´", "Øµ", "Ø¶", "Ø·", "Ø¸", "Ø¹", "Øº", 
                "Ù", "Ù‚", "Ùƒ", "Ù„", "Ù…", "Ù†", "Ù‡Ù€", "Ùˆ", "ÙŠ", 
                "Ù‰", "Ø¦", "Ø¤"
            )
        }
    }

    fun cleanup() {
        stopRecording()
        interpreter?.close()
        interpreter = null
        Log.d(TAG, "ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
    }

    companion object {
        private const val TAG = "SpeechRecognizer"
    }
    
    private data class ProcessedAudio(
        val data: Array<FloatArray>,
        val shape: IntArray
    )
}
